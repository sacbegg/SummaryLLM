{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4ALL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '/Users/sacbe/GPT4All/chat/gpt4all-lora-quantized_LlamaNew.bin'  # replace with your desired local file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: loading model from '/Users/sacbe/GPT4All/chat/gpt4all-lora-quantized_LlamaNew.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32001\n",
      "llama_model_load: n_ctx   = 512\n",
      "llama_model_load: n_embd  = 4096\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 32\n",
      "llama_model_load: n_layer = 32\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 11008\n",
      "llama_model_load: n_parts = 1\n",
      "llama_model_load: type    = 1\n",
      "llama_model_load: ggml map size = 4017.70 MB\n",
      "llama_model_load: ggml ctx size =  81.25 KB\n",
      "llama_model_load: mem required  = 5809.78 MB (+ 2052.00 MB per state)\n",
      "llama_model_load: loading tensors from '/Users/sacbe/GPT4All/chat/gpt4all-lora-quantized_LlamaNew.bin'\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callback_manager=callback_manager, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_generate: seed = 1681149999\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n",
      "generate: n_ctx = 512, n_batch = 1, n_predict = 256, n_keep = 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: make a joke\n",
      "Answer: Tell me if I'm being too sensitive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [end of text]\n",
      "\n",
      "llama_print_timings:        load time = 10644.44 ms\n",
      "llama_print_timings:      sample time =    25.06 ms /    11 runs   (    2.28 ms per run)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
      "llama_print_timings:        eval time =  5477.20 ms /    20 runs   (  273.86 ms per run)\n",
      "llama_print_timings:       total time = 15136.54 ms\n"
     ]
    }
   ],
   "source": [
    "question = \"make a joke\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama model 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/sacbe/Llama_cpp/models/ggml-model-q4_0.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 2\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113739.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "local_path = \"/Users/sacbe/Llama_cpp/models/ggml-model-q4_0.bin\"\n",
    "llm = LlamaCpp(model_path=local_path)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1830.01 ms\n",
      "llama_print_timings:      sample time =   269.24 ms /   114 runs   (    2.36 ms per run)\n",
      "llama_print_timings: prompt eval time =  6276.46 ms /    34 tokens (  184.60 ms per token)\n",
      "llama_print_timings:        eval time = 25777.45 ms /   113 runs   (  228.12 ms per run)\n",
      "llama_print_timings:       total time = 32342.96 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" First, what is Justin's birthyear? (Justin was born in 1982) Then, what was the name of the winner of the Super Bowl held in the year Justin was born? (The winner of Super Bowl XXVII, held in 1983, was the Miami Dolphins.) Finally, what NFL team did the Miami Dolphins play for? (The Dallas Cowboys.) So, the answer is: The Dallas Cowboys won the Super Bowl in the year Justin Biber was born.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/sacbe/GPT4All/chat/gpt4all-lora-quantized.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 2\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "llama_init_from_file: kv self size  =  512.00 MB\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "llama = LlamaCppEmbeddings(model_path=local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"/Users/sacbe/Documents/Summary_LangChain/IA.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-10 11:40:45,312] {posthog.py:15} INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "[2023-04-10 11:40:45,313] {__init__.py:80} INFO - Running Chroma using direct local API.\n",
      "[2023-04-10 11:40:45,759] {__init__.py:49} WARNING - Using embedded DuckDB without persistence: data will be transient\n",
      "[2023-04-10 11:40:45,773] {ctypes.py:22} INFO - Successfully imported ClickHouse Connect C data optimizations\n",
      "[2023-04-10 11:40:45,775] {ctypes.py:31} INFO - Successfully import ClickHouse Connect C/Numpy optimizations\n",
      "[2023-04-10 11:40:45,780] {json_impl.py:45} INFO - Using python library for writing JSON byte strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1446.88 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 40794.30 ms /   237 tokens (  172.13 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 40796.86 ms\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import LatexTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = LatexTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "# select which embeddings we want to use\n",
    "embeddings = llama\n",
    "# create the vectorestore to use as the index\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1446.88 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1453.09 ms /     8 tokens (  181.64 ms per token)\n",
      "llama_print_timings:        eval time =   276.42 ms /     1 runs   (  276.42 ms per run)\n",
      "llama_print_timings:       total time =  1730.26 ms\n",
      "llama_generate: seed = 1681148494\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n",
      "generate: n_ctx = 512, n_batch = 1, n_predict = 256, n_keep = 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Una Inteligencia Artificial (IA) es un sistema de computación diseñado para aprender y tomar decisiones por sí mismo. Para hacerlo, utiliza algoritmos matemáticos y modelos estadísticos que analizan grandes cantidades de datos para identificar patrones y relaciones entre ellos.Una IA se entrena mediante el proceso de aprendizaje automático, donde se le proporciona datos de entrada y se le enseña a asociarlos con resultados específicos. Con el tiempo, la IA aprende a hacer predicciones y tomar decisiones basadas en la información que ha sido alimentada.Las IA se utilizan en una amplia gama de aplicaciones, desde asistentes virtuales y chatbots hasta sistemas de diagnóstico médico y vehículos autónomos. A medida que la tecnología avanza, se espera que las IA sean cada vez más sofisticadas y capaces de realizar tareas cada vez más complejas.\n",
      "\n",
      "Question: Dame un resumen del texto\n",
      "Helpful Answer: Este es el contexto para una respuesta a este cuestionario sobre inteligencia artificial (IA). Para que pueda utilizarse IA, se requiere la información por parte de algoritmos matemáticos y modelos estadísticos. Estas fórmulas proporcionan patrones entre los datos disponibles como resultado del aprendizaje automático para entrenar una máquina con capacidades informativas más altamente organizadas que es capaz asociarse de información para tomar decisiones basándose en las respuestas.\n",
      "Las IA son usados por múltiples aplicaciones, desde asistentes virtuales y chatbots hasta sistemas médicos diagnóstico e vehículos autónomos. A medida que la tecnología progresa más hacia adelante en el tiempo se esperan los diálogos con cada vez mayores complejidad.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [end of text]\n",
      "\n",
      "llama_print_timings:        load time = 191860.62 ms\n",
      "llama_print_timings:      sample time =   419.12 ms /   230 runs   (    1.82 ms per run)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
      "llama_print_timings:        eval time = 180431.45 ms /   544 runs   (  331.68 ms per run)\n",
      "llama_print_timings:       total time = 474889.46 ms\n"
     ]
    }
   ],
   "source": [
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm, chain_type=\"stuff\", retriever = retriever, return_source_documents=False)\n",
    "query = \"Dame un resumen del texto\"\n",
    "result = qa({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1446.88 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4903.21 ms /    11 tokens (  445.75 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4904.08 ms\n",
      "llama_generate: seed = 1681148800\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n",
      "generate: n_ctx = 512, n_batch = 1, n_predict = 256, n_keep = 0\n",
      "\n",
      "\n",
      " [end of text]\n",
      "\n",
      "llama_print_timings:        load time = 191860.62 ms\n",
      "llama_print_timings:      sample time =   604.56 ms /   333 runs   (    1.82 ms per run)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
      "llama_print_timings:        eval time = 314543.49 ms /   946 runs   (  332.50 ms per run)\n",
      "llama_print_timings:       total time = 749108.86 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"De que habla el último parrafo?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Una Inteligencia Artificial (IA) es un sistema de computación diseñado para aprender y tomar decisiones por sí mismo. Para hacerlo, utiliza algoritmos matemáticos y modelos estadísticos que analizan grandes cantidades de datos para identificar patrones y relaciones entre ellos.Una IA se entrena mediante el proceso de aprendizaje automático, donde se le proporciona datos de entrada y se le enseña a asociarlos con resultados específicos. Con el tiempo, la IA aprende a hacer predicciones y tomar decisiones basadas en la información que ha sido alimentada.Las IA se utilizan en una amplia gama de aplicaciones, desde asistentes virtuales y chatbots hasta sistemas de diagnóstico médico y vehículos autónomos. A medida que la tecnología avanza, se espera que las IA sean cada vez más sofisticadas y capaces de realizar tareas cada vez más complejas.\n",
      "\n",
      "Question: De que habla el último parrafo?\n",
      "Helpful Answer: Las últimas líneas dan una perspectiva general sobre lo que un AI es, su historia pasada y sus aplicaciones actuales en diferentes áreas como asistentes virtuales o vehículos autónomos. Sin embargo, son sólo dos de las muchísimas posibilidades con estas tecnologías prometidas para cambiar nuestra vida cotidiana por completo (y tal vez inclinarla hacia un futuro más inteligente).\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
